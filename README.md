# ASL-to-text

The objective of this project was to help the speech and hearing impaired. The proposed model can recognise American Sign Language (ASL) gestures using Image Recognition and display the message on a screen thereby allowing them to communicate with those not familiar with ASL.  
The model could also be adapted to the sign languages of other countries after being trained on the appropriate datasets.

![NIDCD-ASL-hands-2014](https://user-images.githubusercontent.com/83747330/123040083-cd11e780-d410-11eb-90c7-82c4fdf0ed3a.jpg)


# Dataset

The dataset that was used was ASL Alphabet available on Kaggle. You can find it here: https://www.kaggle.com/grassknoted/asl-alphabet
The dataset is a collection of images of alphabets from the American Sign Language, separated in 29 folders, of which 26 are for the letters A-Z and the rest for SPACE, DELETE and NOTHING. The training dataset contains a total of 87000 images which are of 200x200 pixels.

![Screenshot (9)](https://user-images.githubusercontent.com/83747330/123039533-f67e4380-d40f-11eb-92db-09b6bfe13e7f.png)

# Plot

![plot](https://user-images.githubusercontent.com/83747330/123040184-faf72c00-d410-11eb-9f4c-1aeadac3ecea.png)

# Result

![Screenshot (10)](https://user-images.githubusercontent.com/83747330/123040256-182bfa80-d411-11eb-8e80-4d5e9d2e7c89.png)
